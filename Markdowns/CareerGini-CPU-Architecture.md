# CareerGini - System Architecture Documentation (CPU-Optimized)

## ğŸ“‹ Table of Contents
1. [System Overview](#system-overview)
2. [Hardware Infrastructure](#hardware-infrastructure)
3. [Architecture Diagram](#architecture-diagram)
4. [Component Architecture](#component-architecture)
5. [Data Flow](#data-flow)
6. [Technology Stack](#technology-stack)
7. [Deployment Architecture](#deployment-architecture)
8. [Performance Optimization](#performance-optimization)

---

## System Overview

**CareerGini** is an AI-powered career advisory platform that uses multi-agent AI with **100% CPU-based local LLMs** to provide personalized career guidance, skill gap analysis, resume generation, and job recommendations.

### Key Capabilities
- Multi-source profile aggregation (LinkedIn, GitHub, Resumes)
- Intelligent skill gap analysis using local AI
- JD-matched resume generation
- Real-time job recommendations
- Conversational career guidance
- Curated learning resource discovery

### Design Philosophy
- **Privacy-First**: 100% local LLM processing (NO external API calls to OpenAI/Anthropic)
- **Cost-Effective**: Zero ongoing AI API costs
- **Scalable**: Optimized for CPU-only inference on affordable cloud infrastructure
- **Production-Ready**: Battle-tested quantized models with proven performance

---

## Hardware Infrastructure

### Recommended Server Configuration

**Primary Deployment: OVH B2-30 or Equivalent**

```yaml
Server Specifications:
  Provider: OVH Cloud / Hetzner Cloud / Any VPS Provider
  Type: B2-30 equivalent (General Purpose, 8 vCPUs, 30 GB RAM)
  
  CPU:
    Cores: 8 vCores (dedicated)
    Architecture: x86_64 (Intel Haswell/Broadwell or AMD EPYC)
    Clock Speed: 2.0-2.4 GHz base
    Recommended: Intel Xeon or AMD EPYC with AVX2 support
    
  RAM:
    Capacity: 30 GB DDR4 ECC
    Channels: Dual-channel (sufficient for 7B models)
    Type: ECC Registered (data integrity)
    
  Storage:
    Type: NVMe SSD
    Capacity: 200 GB minimum
    Breakdown:
      - OS + System: 20 GB
      - Docker Images: 30 GB
      - LLM Models: ~12 GB (quantized)
      - PostgreSQL Data: 50 GB
      - Application Files: 20 GB
      - Logs & Cache: 10 GB
      - Buffer: 58 GB
      
  Network:
    Bandwidth: 500 Mbps minimum
    Inbound: Unlimited
    Outbound: Unlimited (check provider limits)
    
  Cost:
    OVH B2-30: â‚¬0.26/hour (~â‚¬187/month, ~â‚¹17,000/month)
    Hetzner CPX41: â‚¬0.17/hour (~â‚¬122/month, ~â‚¹11,000/month)
    Alternative: AWS t3.2xlarge spot instances (~$80-120/month)
```

### Why This Configuration Works

**CPU-Only Inference Viability:**
- Modern quantized models (Q4_K_M) achieve 15-25 tokens/second on 8-core CPUs
- 30 GB RAM sufficient for 7B parameter models with full context
- No GPU required - reduces cost by 70-80% vs GPU instances
- Proven performance: Llama 2 7B Q4_K_M reaches 15 t/s on AMD EPYC

**Memory Allocation:**
```
Total: 30 GB RAM
â”œâ”€ OS & System: 2 GB
â”œâ”€ Ollama Service: 16 GB (model loading + inference)
â”œâ”€ PostgreSQL: 4 GB
â”œâ”€ Redis: 2 GB
â”œâ”€ API Services: 3 GB
â”œâ”€ Frontend: 1 GB
â””â”€ Buffer: 2 GB
```

---

## Architecture Diagram

### High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLIENT LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           React SPA (TypeScript + Vite)                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚   Chat   â”‚  â”‚ Profile  â”‚  â”‚  Resume  â”‚  â”‚   Jobs   â”‚    â”‚  â”‚
â”‚  â”‚  â”‚Interface â”‚  â”‚Dashboard â”‚  â”‚  Builder â”‚  â”‚  Board   â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  State Management: Zustand | API Client: React Query        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ HTTPS/WebSocket
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          API GATEWAY LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Node.js/Express API Gateway (Port 3000)              â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚  â”‚ OAuth Managerâ”‚  â”‚ JWT Handler  â”‚  â”‚Rate Limiter  â”‚      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚         Request Router & Load Balancer              â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚               â”‚               â”‚
                â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MICROSERVICES LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  AI Service      â”‚  â”‚ Profile Service  â”‚  â”‚  Job Service     â”‚ â”‚
â”‚  â”‚  (Python/FastAPI)â”‚  â”‚ (Node.js/Express)â”‚  â”‚(Node.js/Express) â”‚ â”‚
â”‚  â”‚   Port 8000      â”‚  â”‚   Port 3001      â”‚  â”‚   Port 3002      â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â”‚  LangGraph   â”‚ â”‚  â”‚ â”‚LinkedIn API  â”‚ â”‚  â”‚ â”‚Job Board API â”‚ â”‚
â”‚  â”‚ â”‚ Orchestrator â”‚ â”‚  â”‚ â”‚GitHub API    â”‚ â”‚  â”‚ â”‚Search Engine â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚Resume Parser â”‚ â”‚  â”‚ â”‚Recommender   â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â”‚ Multi-Agent  â”‚ â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â”‚   System     â”‚ â”‚  â”‚ â”‚Profile       â”‚ â”‚  â”‚ â”‚Application   â”‚ â”‚
â”‚  â”‚ â”‚              â”‚ â”‚  â”‚ â”‚Aggregator    â”‚ â”‚  â”‚ â”‚Tracker       â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Supervisor  â”‚ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ â”‚â€¢ Profile     â”‚ â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Skills Gap  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ â”‚â€¢ Job Search  â”‚ â”‚                                             â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Resume      â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Learning    â”‚ â”‚  â”‚Learning Service  â”‚                      â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚(Node.js/Express) â”‚                      â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚   Port 3003      â”‚                      â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚                  â”‚                      â”‚ â”‚
â”‚  â”‚ â”‚Ollama Client â”‚ â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                      â”‚ â”‚
â”‚  â”‚ â”‚Integration   â”‚ â”‚  â”‚ â”‚YouTube API   â”‚ â”‚                      â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚GitHub Search â”‚ â”‚                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚Course APIs   â”‚ â”‚                      â”‚ â”‚
â”‚           â”‚            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                      â”‚ â”‚
â”‚           â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚ â”‚
â”‚           â”‚                                                       â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LOCAL LLM INFERENCE LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Ollama Service (Port 11434)                           â”‚  â”‚
â”‚  â”‚         CPU-Optimized Inference Engine                        â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  Loaded Models (Q4_K_M Quantization)                   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Qwen2.5:7b-instruct-q4_K_M (~4.5 GB)               â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Supervisor, Resume Builder                        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Benchmark: 18-22 t/s, 74.2 MMLU                 â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Phi3:3.8b-mini-instruct-q4_K_M (~2.5 GB)          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Profile Analysis, Job Search, Learning           â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Benchmark: 28-32 t/s, fast inference            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Qwen2.5-Coder:7b-instruct-q4_K_M (~4.5 GB)        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Skills Gap Analysis (technical tasks)            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Benchmark: 20-25 t/s, 57.9 HumanEval            â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  Configuration:                                              â”‚  â”‚
â”‚  â”‚  - OLLAMA_NUM_THREADS=6 (75% of 8 vCores)                  â”‚  â”‚
â”‚  â”‚  - OLLAMA_NUM_GPU=0 (CPU-only mode)                        â”‚  â”‚
â”‚  â”‚  - OLLAMA_MAX_LOADED_MODELS=2 (memory optimization)        â”‚  â”‚
â”‚  â”‚  - Context Length: 2048 tokens (balanced)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA LAYER                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   PostgreSQL     â”‚  â”‚   ChromaDB       â”‚  â”‚     Redis        â”‚ â”‚
â”‚  â”‚  (Primary DB)    â”‚  â”‚ (Vector Store)   â”‚  â”‚   (Cache/Queue)  â”‚ â”‚
â”‚  â”‚   Port 5432      â”‚  â”‚   Port 8001      â”‚  â”‚   Port 6379      â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â€¢ Users          â”‚  â”‚ â€¢ Profile        â”‚  â”‚ â€¢ Sessions       â”‚ â”‚
â”‚  â”‚ â€¢ Profiles       â”‚  â”‚   Embeddings     â”‚  â”‚ â€¢ Chat History   â”‚ â”‚
â”‚  â”‚ â€¢ Conversations  â”‚  â”‚ â€¢ Job            â”‚  â”‚ â€¢ Rate Limits    â”‚ â”‚
â”‚  â”‚ â€¢ Jobs           â”‚  â”‚   Embeddings     â”‚  â”‚ â€¢ Task Queue     â”‚ â”‚
â”‚  â”‚ â€¢ Applications   â”‚  â”‚ â€¢ Skill          â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â€¢ Resumes        â”‚  â”‚   Embeddings     â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ Storage: 50 GB   â”‚  â”‚ Storage: 10 GB   â”‚  â”‚ Memory: 2 GB     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXTERNAL INTEGRATIONS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ OAuth        â”‚  â”‚  Job Boards  â”‚  â”‚  Learning    â”‚             â”‚
â”‚  â”‚ Providers    â”‚  â”‚              â”‚  â”‚  Platforms   â”‚             â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚             â”‚
â”‚  â”‚ â€¢ Google     â”‚  â”‚ â€¢ LinkedIn   â”‚  â”‚ â€¢ YouTube    â”‚             â”‚
â”‚  â”‚ â€¢ GitHub     â”‚  â”‚ â€¢ Indeed     â”‚  â”‚ â€¢ GitHub     â”‚             â”‚
â”‚  â”‚ â€¢ LinkedIn   â”‚  â”‚ â€¢ Glassdoor  â”‚  â”‚ â€¢ Coursera   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Architecture

### 1. Frontend Architecture

**Technology**: React 18 + TypeScript + Vite

```
Frontend (Port 5173)
â”‚
â”œâ”€â”€ Presentation Layer
â”‚   â”œâ”€â”€ Chat Interface (Real-time streaming)
â”‚   â”œâ”€â”€ Profile Dashboard (Data visualization)
â”‚   â”œâ”€â”€ Resume Builder (Live preview)
â”‚   â”œâ”€â”€ Job Board (Filtering & recommendations)
â”‚   â””â”€â”€ Learning Hub (Resource curation)
â”‚
â”œâ”€â”€ State Management (Zustand)
â”‚   â”œâ”€â”€ Auth Store (JWT, user session)
â”‚   â”œâ”€â”€ Chat Store (Messages, streaming)
â”‚   â”œâ”€â”€ Profile Store (User data, skills)
â”‚   â””â”€â”€ UI Store (Modals, notifications)
â”‚
â””â”€â”€ API Integration (React Query + Axios)
    â”œâ”€â”€ HTTP Client (REST APIs)
    â”œâ”€â”€ WebSocket Client (Chat streaming)
    â””â”€â”€ Caching Layer (React Query)
```

### 2. API Gateway Architecture

**Technology**: Node.js + Express

```
API Gateway (Port 3000)
â”‚
â”œâ”€â”€ Authentication Layer
â”‚   â”œâ”€â”€ OAuth 2.0 Flows (Google, GitHub, LinkedIn)
â”‚   â”œâ”€â”€ JWT Generation & Validation
â”‚   â””â”€â”€ Session Management (Redis)
â”‚
â”œâ”€â”€ Security Layer
â”‚   â”œâ”€â”€ Rate Limiting (Redis-backed)
â”‚   â”œâ”€â”€ Request Validation (Joi)
â”‚   â”œâ”€â”€ CORS Configuration
â”‚   â””â”€â”€ Helmet Security Headers
â”‚
â””â”€â”€ Routing Layer
    â”œâ”€â”€ /api/v1/auth/* â†’ OAuth endpoints
    â”œâ”€â”€ /api/v1/chat/* â†’ AI Service
    â”œâ”€â”€ /api/v1/profile/* â†’ Profile Service
    â”œâ”€â”€ /api/v1/jobs/* â†’ Job Service
    â””â”€â”€ /api/v1/learning/* â†’ Learning Service
```

### 3. AI Service Architecture (CPU-Optimized)

**Technology**: Python + FastAPI + LangGraph + Ollama

```
AI Service (Port 8000)
â”‚
â”œâ”€â”€ Ollama Client Layer
â”‚   â”œâ”€â”€ Model Manager
â”‚   â”‚   â”œâ”€â”€ Load/Unload Models
â”‚   â”‚   â”œâ”€â”€ Memory Optimization
â”‚   â”‚   â””â”€â”€ Thread Configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ Inference Engine
â”‚   â”‚   â”œâ”€â”€ Qwen2.5 7B (Reasoning)
â”‚   â”‚   â”œâ”€â”€ Phi3 Mini (Fast Tasks)
â”‚   â”‚   â””â”€â”€ Qwen2.5-Coder 7B (Technical)
â”‚   â”‚
â”‚   â””â”€â”€ Context Manager
â”‚       â”œâ”€â”€ Token Counting
â”‚       â”œâ”€â”€ Context Window (2048 tokens)
â”‚       â””â”€â”€ Memory Buffer Management
â”‚
â”œâ”€â”€ LangGraph Orchestration Layer
â”‚   â”œâ”€â”€ State Graph Definition
â”‚   â”œâ”€â”€ Node Registration (6 agents)
â”‚   â”œâ”€â”€ Edge Routing Logic
â”‚   â””â”€â”€ Conditional Branching
â”‚
â”œâ”€â”€ Multi-Agent System
â”‚   â”œâ”€â”€ Supervisor Agent
â”‚   â”‚   â””â”€â”€ Uses: Qwen2.5 7B (complex routing)
â”‚   â”‚
â”‚   â”œâ”€â”€ Profile Analysis Agent
â”‚   â”‚   â””â”€â”€ Uses: Phi3 Mini (fast extraction)
â”‚   â”‚
â”‚   â”œâ”€â”€ Skills Gap Agent
â”‚   â”‚   â””â”€â”€ Uses: Qwen2.5-Coder 7B (technical analysis)
â”‚   â”‚
â”‚   â”œâ”€â”€ Job Search Agent
â”‚   â”‚   â””â”€â”€ Uses: Phi3 Mini (fast matching)
â”‚   â”‚
â”‚   â”œâ”€â”€ Resume Builder Agent
â”‚   â”‚   â””â”€â”€ Uses: Qwen2.5 7B (content generation)
â”‚   â”‚
â”‚   â””â”€â”€ Learning Resource Agent
â”‚       â””â”€â”€ Uses: Phi3 Mini (curation)
â”‚
â”œâ”€â”€ Vector Store Integration
â”‚   â”œâ”€â”€ ChromaDB Client
â”‚   â”œâ”€â”€ Embedding Generation (sentence-transformers)
â”‚   â”‚   â””â”€â”€ Model: all-MiniLM-L6-v2 (local, CPU-optimized)
â”‚   â””â”€â”€ Similarity Search
â”‚
â””â”€â”€ API Endpoints
    â”œâ”€â”€ POST /chat (Main conversation endpoint)
    â”œâ”€â”€ POST /resume/generate (JD-matched resume)
    â”œâ”€â”€ POST /skills/analyze (Skills gap analysis)
    â”œâ”€â”€ GET /jobs/recommend (Job recommendations)
    â””â”€â”€ GET /learning/resources (Curated resources)
```

**Critical Ollama Integration Details:**

```python
# integrations/ollama_client.py
from langchain_community.chat_models import ChatOllama
import os

class OllamaClient:
    def __init__(self):
        self.base_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
        
        # Model for complex reasoning (supervisor, resume)
        self.llm_reasoning = ChatOllama(
            model="qwen2.5:7b-instruct-q4_K_M",
            base_url=self.base_url,
            temperature=0.7,
            num_ctx=2048,      # Context window
            num_thread=6       # 75% of 8 vCores
        )
        
        # Model for fast tasks (profile, jobs, learning)
        self.llm_fast = ChatOllama(
            model="phi3:3.8b-mini-instruct-q4_K_M",
            base_url=self.base_url,
            temperature=0.3,
            num_ctx=2048,
            num_thread=6
        )
        
        # Model for technical/coding tasks (skills gap)
        self.llm_coder = ChatOllama(
            model="qwen2.5-coder:7b-instruct-q4_K_M",
            base_url=self.base_url,
            temperature=0.2,
            num_ctx=2048,
            num_thread=6
        )
    
    def get_model(self, task_type: str):
        """Return appropriate model based on task complexity"""
        if task_type in ["reasoning", "routing", "complex", "resume"]:
            return self.llm_reasoning
        elif task_type in ["coding", "technical", "skills"]:
            return self.llm_coder
        else:
            return self.llm_fast
```

### 4. Profile Service Architecture

```
Profile Service (Port 3001)
â”‚
â”œâ”€â”€ LinkedIn Integration
â”‚   â”œâ”€â”€ OAuth Flow
â”‚   â”œâ”€â”€ Profile Fetching
â”‚   â””â”€â”€ Experience Extraction
â”‚
â”œâ”€â”€ GitHub Integration
â”‚   â”œâ”€â”€ Token Authentication
â”‚   â”œâ”€â”€ Repository Analysis
â”‚   â””â”€â”€ Language Detection
â”‚
â”œâ”€â”€ Resume Parser
â”‚   â”œâ”€â”€ PDF Text Extraction (pdf-parse)
â”‚   â”œâ”€â”€ AI-based Structuring (Ollama)
â”‚   â””â”€â”€ Data Validation
â”‚
â””â”€â”€ Profile Aggregator
    â”œâ”€â”€ Data Merging
    â”œâ”€â”€ Skill Deduplication
    â””â”€â”€ Database Storage
```

### 5. Job & Learning Services

```
Job Service (Port 3002)
â”‚
â”œâ”€â”€ Job Scraper (Playwright)
â”‚   â”œâ”€â”€ LinkedIn Jobs
â”‚   â”œâ”€â”€ Indeed
â”‚   â””â”€â”€ Glassdoor
â”‚
â”œâ”€â”€ Job Matcher (Ollama-powered)
â”‚   â”œâ”€â”€ Skill Matching
â”‚   â”œâ”€â”€ Experience Matching
â”‚   â””â”€â”€ Relevance Scoring
â”‚
â””â”€â”€ Application Tracker
    â””â”€â”€ Status Management

Learning Service (Port 3003)
â”‚
â”œâ”€â”€ YouTube API Integration
â”œâ”€â”€ GitHub Repository Search
â”œâ”€â”€ Course Aggregation (Coursera, freeCodeCamp)
â””â”€â”€ Resource Ranking (Ollama-powered)
```

---

## Data Flow

### 1. User Onboarding Flow

```
User Registration
    â†“
OAuth Provider Selection (Google/GitHub/LinkedIn)
    â†“
API Gateway: JWT Token Generation
    â†“
Frontend: Store Token (Zustand + LocalStorage)
    â†“
Profile Service: Create User Record (PostgreSQL)
    â†“
Dashboard: Display Onboarding Steps
```

### 2. Profile Sync Flow

```
User Initiates LinkedIn/GitHub Sync
    â†“
Profile Service: Fetch Data via APIs
    â†“
Profile Service: Send to AI Service for Parsing
    â†“
AI Service: Ollama (Phi3 Mini) Extracts Structured Data
    â†“
Profile Service: Store in PostgreSQL
    â†“
AI Service: Generate Embeddings (sentence-transformers)
    â†“
ChromaDB: Store Profile Vector
    â†“
Frontend: Display Aggregated Profile
```

### 3. Chat Conversation Flow (Multi-Agent)

```
User Sends Message via Chat Interface
    â†“
API Gateway: Validate JWT, Forward to AI Service
    â†“
AI Service: LangGraph Supervisor Agent
    â”‚         (Ollama - Qwen2.5 7B)
    â†“
Supervisor Routes to Appropriate Agent:
    â”‚
    â”œâ”€â†’ Profile Agent (Phi3 Mini) â†’ Fetch User Data
    â”œâ”€â†’ Skills Gap Agent (Qwen2.5-Coder) â†’ Analyze Skills
    â”œâ”€â†’ Job Search Agent (Phi3 Mini) â†’ Match Jobs
    â”œâ”€â†’ Resume Builder Agent (Qwen2.5 7B) â†’ Generate Resume
    â””â”€â†’ Learning Agent (Phi3 Mini) â†’ Curate Resources
    â†“
Agents Execute Tasks (Parallel where possible)
    â†“
Results Aggregated by Supervisor
    â†“
AI Service: Stream Response to Frontend (SSE/WebSocket)
    â†“
Frontend: Display Streaming Message
    â†“
Chat Store: Save to Conversation History
    â†“
PostgreSQL: Persist Conversation
```

### 4. Resume Generation Flow

```
User Provides Job Description (JD)
    â†“
AI Service: Resume Builder Agent (Ollama - Qwen2.5 7B)
    â†“
Fetch User Profile from PostgreSQL
    â†“
Extract JD Keywords using AI
    â†“
Match Profile Skills to JD Requirements
    â†“
Generate Tailored Resume Content
    â†“
Apply Template Formatting
    â†“
Return Structured Resume JSON
    â†“
Frontend: Render Live Preview
    â†“
User Downloads (PDF/DOCX generation)
```

---

## Technology Stack

### Frontend Stack

| Category | Technology | Version | Purpose |
|----------|-----------|---------|---------|
| **Framework** | React | 18.3.x | UI library |
| **Language** | TypeScript | 5.x | Type safety |
| **Build Tool** | Vite | 5.x | Fast dev server |
| **State** | Zustand | 4.x | Global state |
| **API Client** | React Query | 5.x | Server state caching |
| **Styling** | TailwindCSS | 3.x | Utility-first CSS |
| **Forms** | React Hook Form | 7.x | Form handling |
| **Validation** | Zod | 3.x | Schema validation |
| **Charts** | Recharts | 2.x | Data visualization |

### Backend Stack

| Category | Technology | Version | Purpose |
|----------|-----------|---------|---------|
| **API Gateway** | Node.js + Express | 20.x / 4.x | HTTP server |
| **AI Service** | Python + FastAPI | 3.11 / 0.104.x | AI endpoints |
| **Microservices** | Node.js + Express | 20.x / 4.x | Business logic |
| **Database** | PostgreSQL | 15.x | Primary data store |
| **Cache** | Redis | 7.x | Session & caching |
| **Vector DB** | ChromaDB | 0.4.x | Embeddings (local, free) |

### AI/ML Stack (CPU-Optimized)

| Component | Technology | Details |
|-----------|-----------|---------|
| **LLM Runtime** | Ollama | 0.1.x (CPU-optimized) |
| **Model 1** | Qwen2.5 7B Instruct | Q4_K_M (~4.5 GB), 74.2 MMLU |
| **Model 2** | Phi3 Mini 3.8B | Q4_K_M (~2.5 GB), 28-32 t/s |
| **Model 3** | Qwen2.5-Coder 7B | Q4_K_M (~4.5 GB), 57.9 HumanEval |
| **Orchestration** | LangGraph | 0.0.x (agent framework) |
| **LangChain** | langchain-community | 0.0.x (Ollama integration) |
| **Embeddings** | sentence-transformers | all-MiniLM-L6-v2 (local) |
| **Vector Store** | ChromaDB Client | Python client |

### DevOps Stack

| Category | Technology | Version | Purpose |
|----------|-----------|---------|---------|
| **Containerization** | Docker | 24.x | Container runtime |
| **Orchestration** | Docker Compose | 2.x | Multi-container |
| **Web Server** | Nginx (optional) | 1.25.x | Reverse proxy |
| **Process Manager** | systemd | - | Service management |

---

## Deployment Architecture

### Docker Compose Services

```yaml
version: '3.8'

services:
  # Frontend
  frontend:
    image: careergini-frontend:latest
    ports: ["5173:5173"]
    depends_on: [api-gateway]
    
  # API Gateway
  api-gateway:
    image: careergini-api-gateway:latest
    ports: ["3000:3000"]
    depends_on: [postgres, redis, ai-service]
    
  # AI Service (CPU-Optimized)
  ai-service:
    image: careergini-ai-service:latest
    ports: ["8000:8000"]
    depends_on: [ollama, chromadb, postgres]
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    
  # Ollama (Local LLM Engine)
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes: [ollama_models:/root/.ollama]
    environment:
      - OLLAMA_NUM_THREADS=6       # 75% of 8 vCores
      - OLLAMA_NUM_GPU=0            # Force CPU-only
      - OLLAMA_MAX_LOADED_MODELS=2  # Memory optimization
    deploy:
      resources:
        limits:
          cpus: '7'                  # Reserve 7 of 8 cores
          memory: 16G                # 16 GB for Ollama
    
  # Profile Service
  profile-service:
    image: careergini-profile-service:latest
    ports: ["3001:3001"]
    depends_on: [postgres]
    
  # Job Service
  job-service:
    image: careergini-job-service:latest
    ports: ["3002:3002"]
    depends_on: [postgres]
    
  # Learning Service
  learning-service:
    image: careergini-learning-service:latest
    ports: ["3003:3003"]
    depends_on: [postgres]
    
  # PostgreSQL
  postgres:
    image: postgres:15
    ports: ["5432:5432"]
    volumes: [postgres_data:/var/lib/postgresql/data]
    environment:
      - POSTGRES_DB=careergini
      - POSTGRES_USER=careergini
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    
  # Redis
  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    volumes: [redis_data:/data]
    
  # ChromaDB (Free Vector Database)
  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    ports: ["8001:8000"]
    volumes: [chroma_data:/chroma/chroma]

volumes:
  postgres_data:
  redis_data:
  ollama_models:
  chroma_data:
```

### Resource Allocation

```
Total Server Resources: 8 vCores, 30 GB RAM, 200 GB SSD

CPU Allocation:
â”œâ”€ Ollama:               7 cores (87.5%)
â”œâ”€ AI Service:           0.5 cores (6%)
â”œâ”€ API Gateway:          0.2 cores (2.5%)
â”œâ”€ Microservices:        0.3 cores (4%)
â””â”€ System/Others:        0.0 cores (0%)

RAM Allocation:
â”œâ”€ Ollama:               16 GB (53%)
â”œâ”€ PostgreSQL:           4 GB (13%)
â”œâ”€ AI Service:           3 GB (10%)
â”œâ”€ Redis:                2 GB (7%)
â”œâ”€ ChromaDB:             2 GB (7%)
â”œâ”€ API Gateway:          1 GB (3%)
â”œâ”€ Microservices:        1 GB (3%)
â””â”€ System:               1 GB (3%)

Storage Allocation:
â”œâ”€ Ollama Models:        ~12 GB (quantized)
â”œâ”€ PostgreSQL Data:      50 GB
â”œâ”€ Docker Images:        30 GB
â”œâ”€ ChromaDB Vectors:     10 GB
â”œâ”€ Application Logs:     10 GB
â”œâ”€ OS & System:          20 GB
â””â”€ Buffer:               68 GB
```

---

## Performance Optimization

### CPU-Based LLM Inference Optimization

**1. Model Selection Strategy:**

```yaml
Task-to-Model Mapping:
  Complex Reasoning (Supervisor, Resume):
    Model: Qwen2.5 7B Q4_K_M
    Performance: 18-22 tokens/second
    Memory: ~4.5 GB
    Quality: 74.2 MMLU (SOTA for 7B)
    
  Fast Tasks (Profile, Jobs, Learning):
    Model: Phi3 Mini 3.8B Q4_K_M
    Performance: 28-32 tokens/second
    Memory: ~2.5 GB
    Quality: Good for extraction/classification
    
  Technical Tasks (Skills Gap):
    Model: Qwen2.5-Coder 7B Q4_K_M
    Performance: 20-25 tokens/second
    Memory: ~4.5 GB
    Quality: 57.9 HumanEval (best coding model)
```

**2. Quantization Strategy:**

```
Why Q4_K_M (4-bit K-means Medium):
âœ“ 70% size reduction vs FP16
âœ“ Minimal quality loss (~92% of FP16)
âœ“ 3.5-4x faster inference on CPU
âœ“ Fits multiple models in 30 GB RAM
âœ“ Mixed precision (6-bit attention, 4-bit FFN)
âœ“ Production-proven (Meta, Hugging Face)

Alternative: Q5_K_M (if quality critical)
- 64% size reduction
- 95% of FP16 quality
- 20% slower than Q4_K_M
```

**3. Thread Configuration:**

```bash
# Optimal thread allocation for 8 vCore system
OLLAMA_NUM_THREADS=6  # Use 75% of cores

Why not 8 threads?
- Leave 2 cores for system + other services
- Prevents resource contention
- Better overall throughput

Benchmarks:
- 4 threads: 15 t/s (75% utilization)
- 6 threads: 20 t/s (90% utilization) âœ“
- 8 threads: 19 t/s (95% utilization, contention)
```

**4. Context Window Optimization:**

```bash
# Balanced context for memory efficiency
num_ctx=2048  # Default: 2048 tokens

Memory Impact:
- 2048 tokens: ~2 GB KV cache
- 4096 tokens: ~4 GB KV cache (doubles memory)
- 8192 tokens: ~8 GB KV cache (quadruples)

Strategy:
- Use 2048 for most tasks (sufficient for career advice)
- Expand to 4096 only for long resume generation
- Never exceed 4096 to avoid OOM
```

**5. Model Loading Strategy:**

```bash
# Load models on-demand, unload unused
OLLAMA_MAX_LOADED_MODELS=2

Strategy:
1. Keep Phi3 Mini loaded (most frequent, 2.5 GB)
2. Load Qwen2.5 7B on-demand (4.5 GB)
3. Unload Qwen2.5-Coder when idle
4. Total active: ~7 GB (leaves 9 GB buffer)
```

### Database Optimization

**PostgreSQL Configuration:**

```sql
-- postgresql.conf optimizations
shared_buffers = 4GB              -- 25% of RAM
effective_cache_size = 12GB       -- 75% of available RAM
work_mem = 64MB                   -- Per operation
maintenance_work_mem = 512MB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
random_page_cost = 1.1            -- SSD optimization
effective_io_concurrency = 200    -- SSD parallel I/O
```

**Indexing Strategy:**

```sql
-- Critical indexes for performance
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_profiles_user_id ON profiles(user_id);
CREATE INDEX idx_jobs_created_at ON jobs(created_at DESC);
CREATE INDEX idx_conversations_user_id ON conversations(user_id);
CREATE INDEX idx_applications_user_job ON applications(user_id, job_id);

-- JSONB indexes for flexible queries
CREATE INDEX idx_profiles_skills_gin ON profiles USING GIN (skills);
CREATE INDEX idx_jobs_requirements_gin ON jobs USING GIN (requirements);
```

### Caching Strategy

**Redis Cache Layers:**

```
1. API Response Cache (TTL: 5 minutes)
   - Job search results
   - Learning resource lists
   
2. Session Cache (TTL: 24 hours)
   - JWT tokens
   - User sessions
   
3. Rate Limit Cache (TTL: 1 minute)
   - Request counts per IP
   - Per-user rate limits
   
4. Chat History Cache (TTL: 1 hour)
   - Recent conversation context
   - Reduces DB queries
```

### Network Optimization

**1. Enable gzip compression:**
```nginx
gzip on;
gzip_types text/plain text/css application/json application/javascript;
gzip_min_length 1000;
```

**2. HTTP/2 support:**
```nginx
listen 443 ssl http2;
```

**3. Connection pooling:**
```javascript
// PostgreSQL connection pool
const pool = new Pool({
  max: 20,              // Max connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});
```

---

## Monitoring & Observability

### Health Checks

```yaml
# Docker Compose health checks
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```

### Logging Strategy

```
Application Logs:
â”œâ”€ API Gateway: Morgan (HTTP logs)
â”œâ”€ AI Service: Python logging (INFO level)
â”œâ”€ Ollama: Container logs
â””â”€ PostgreSQL: Query logs (slow queries only)

Log Rotation:
- Max size: 100 MB per file
- Keep: 7 days of logs
- Compression: gzip old logs
```

### Performance Metrics

**Key Metrics to Monitor:**

```
1. LLM Inference:
   - Tokens per second (target: 18-25 t/s)
   - Model load time (target: <5s)
   - Memory usage (target: <16 GB)
   
2. API Response Times:
   - p50: <200ms
   - p95: <500ms
   - p99: <1000ms
   
3. Database:
   - Connection pool usage (<80%)
   - Query execution time (<100ms average)
   - Cache hit rate (>90%)
   
4. System:
   - CPU usage (<85% average)
   - RAM usage (<90%)
   - Disk I/O (<70% utilization)
```

---

## Security Considerations

### Authentication & Authorization

```
1. OAuth 2.0 Implementation:
   - Secure token exchange
   - State parameter for CSRF protection
   - Redirect URI validation
   
2. JWT Token Security:
   - HS256 signing algorithm
   - 15-minute access token expiry
   - 7-day refresh token expiry
   - Secure httpOnly cookies
   
3. API Security:
   - Rate limiting (100 req/min per IP)
   - Input validation (Joi schemas)
   - SQL injection prevention (parameterized queries)
   - XSS protection (sanitize inputs)
```

### Data Privacy

```
1. Local LLM Processing:
   - NO data sent to external APIs
   - 100% on-premise inference
   - User data stays on server
   
2. Database Encryption:
   - At-rest encryption (disk level)
   - SSL/TLS for connections
   - Encrypted backups
   
3. GDPR Compliance:
   - User data deletion on request
   - Data export functionality
   - Consent management
```

---

## Scaling Strategy

### Vertical Scaling (Phase 1)

```
Current: B2-30 (8 vCores, 30 GB RAM)
    â†“
Upgrade Path:
â”œâ”€ B2-60: 16 vCores, 60 GB RAM (~â‚¹34,000/month)
â”‚   â””â”€ Supports 13B models, 2x throughput
â”‚
â””â”€ B2-120: 32 vCores, 120 GB RAM (~â‚¹68,000/month)
    â””â”€ Supports 70B models with quantization
```

### Horizontal Scaling (Phase 2)

```
Load Balancer
    â”‚
    â”œâ”€â†’ API Gateway 1 (B2-15)
    â”œâ”€â†’ API Gateway 2 (B2-15)
    â”‚
    â”œâ”€â†’ AI Service 1 (B2-30 with Ollama)
    â”œâ”€â†’ AI Service 2 (B2-30 with Ollama)
    â”‚
    â””â”€â†’ Shared: PostgreSQL (RDS), Redis (Managed)
```

---

## Cost Analysis

### Monthly Operating Costs (Estimated)

```
Infrastructure:
â”œâ”€ OVH B2-30 Server:           â‚¬187/month (~â‚¹17,000)
â”œâ”€ Domain + SSL:               â‚¬5/month (~â‚¹450)
â”œâ”€ Backups (100 GB):           â‚¬10/month (~â‚¹900)
â”œâ”€ Monitoring (optional):      â‚¬0/month (self-hosted)
â””â”€ TOTAL INFRASTRUCTURE:       â‚¬202/month (~â‚¹18,350)

External APIs:
â”œâ”€ YouTube Data API:           Free (10K req/day)
â”œâ”€ GitHub API:                 Free (5K req/hour)
â”œâ”€ LinkedIn API:               Free (basic)
â””â”€ TOTAL EXTERNAL APIS:        â‚¬0/month

LLM Costs:
â”œâ”€ Ollama (Local):             â‚¬0/month âœ“
â”œâ”€ OpenAI (Alternative):       â‚¬200-500/month âŒ
â””â”€ TOTAL LLM COSTS:            â‚¬0/month

GRAND TOTAL:                   â‚¬202/month (~â‚¹18,350)

Cost Comparison (100K requests/month):

CareerGini (Local LLMs):       â‚¬202/month
Alternative (OpenAI GPT-4):    â‚¬1,200/month
Alternative (Anthropic Claude): â‚¬900/month

Savings: 83% cheaper than paid LLM APIs
```

---

## Deployment Checklist

### Pre-Deployment

- [ ] Server provisioned (B2-30 or equivalent)
- [ ] Docker + Docker Compose installed
- [ ] Domain configured with DNS
- [ ] SSL certificate obtained (Let's Encrypt)
- [ ] Environment variables configured
- [ ] OAuth apps created (Google, GitHub, LinkedIn)
- [ ] External API keys obtained
- [ ] Backup strategy configured
- [ ] Monitoring setup (optional)

### Initial Deployment

```bash
# 1. Clone repository
git clone https://github.com/your-org/careergini.git
cd careergini

# 2. Configure environment
cp .env.example .env
nano .env  # Edit with your values

# 3. Download Ollama models
make models  # Downloads ~12 GB of models

# 4. Start services
make dev  # Development mode
# OR
make deploy  # Production mode

# 5. Initialize database
make db-init

# 6. Verify deployment
curl http://localhost:3000/health
curl http://localhost:8000/health
curl http://localhost:11434/api/tags
```

### Post-Deployment

- [ ] Smoke test all features
- [ ] Load test API endpoints
- [ ] Verify LLM inference performance
- [ ] Test OAuth flows
- [ ] Check database connections
- [ ] Monitor resource usage (first 24 hours)
- [ ] Configure automated backups
- [ ] Setup monitoring alerts

---

## Troubleshooting

### Common Issues

**1. Ollama out of memory:**
```bash
# Solution: Reduce loaded models
OLLAMA_MAX_LOADED_MODELS=1

# Or reduce context window
num_ctx=1024
```

**2. Slow inference (<10 t/s):**
```bash
# Check thread configuration
OLLAMA_NUM_THREADS=6  # Should be 75% of cores

# Verify no GPU fallback
OLLAMA_NUM_GPU=0
```

**3. Database connection pool exhausted:**
```sql
-- Increase max connections
max_connections = 200  -- postgresql.conf
```

**4. High memory usage:**
```bash
# Check container memory limits
docker stats

# Restart services to free memory
docker-compose restart
```

---

## References

[1] llama.cpp CPU Inference Benchmarks: https://github.com/ggerganov/llama.cpp  
[2] Ollama Performance Documentation: https://ollama.com/blog/cpu-inference  
[3] Qwen2.5 Speed Benchmarks: https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html  
[4] Phi3 Technical Report: https://arxiv.org/abs/2404.14219  
[5] LangGraph Documentation: https://langchain-ai.github.io/langgraph/  
[6] ChromaDB Documentation: https://docs.trychroma.com/  

---

**Document Version**: 2.0 (CPU-Optimized for B2-30)  
**Last Updated**: February 17, 2026  
**Maintained By**: CareerGini Development Team