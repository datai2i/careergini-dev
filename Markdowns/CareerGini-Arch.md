# CareerGini - System Architecture Documentation (CPU-Optimized)

## ğŸ“‹ Table of Contents
1. [System Overview](#system-overview)
2. [Hardware Infrastructure](#hardware-infrastructure)
3. [Architecture Diagram](#architecture-diagram)
4. [Component Architecture](#component-architecture)
5. [Data Flow](#data-flow)
6. [Technology Stack](#technology-stack)
7. [Deployment Architecture](#deployment-architecture)
8. [Performance Optimization](#performance-optimization)

---

## System Overview

**CareerGini** is an AI-powered career advisory platform that uses multi-agent AI with **100% CPU-based local LLMs** to provide personalized career guidance, skill gap analysis, resume generation, and job recommendations.

### Key Capabilities
- Multi-source profile aggregation (LinkedIn, GitHub, Resumes)
- Intelligent skill gap analysis using local AI
- JD-matched resume generation
- Real-time job recommendations
- Conversational career guidance
- Curated learning resource discovery

### Design Philosophy
- **Privacy-First**: 100% local LLM processing (NO external API calls to OpenAI/Anthropic)
- **Cost-Effective**: Zero ongoing AI API costs
- **Scalable**: Optimized for CPU-only inference on affordable cloud infrastructure
- **Production-Ready**: Battle-tested quantized models with proven performance

---

## Hardware Infrastructure

### Recommended Server Configuration

**Primary Deployment: OVH B2-30 or Equivalent**

```
Server Specifications:
  Provider: OVH Cloud / Hetzner Cloud / Any VPS Provider
  Type: B2-30 equivalent (General Purpose, 8 vCPUs, 30 GB RAM)
  
  CPU:
    Cores: 8 vCores (dedicated)
    Architecture: x86_64 (Intel Haswell/Broadwell or AMD EPYC)
    Clock Speed: 2.0-2.4 GHz base
    Recommended: Intel Xeon or AMD EPYC with AVX2 support
    
  RAM:
    Capacity: 30 GB DDR4 ECC
    Channels: Dual-channel (sufficient for 7B models)
    Type: ECC Registered (data integrity)
    
  Storage:
    Type: NVMe SSD
    Capacity: 200 GB minimum
    Breakdown:
      - OS + System: 20 GB
      - Docker Images: 30 GB
      - LLM Models: ~12 GB (quantized)
      - PostgreSQL Data: 50 GB
      - Application Files: 20 GB
      - Logs & Cache: 10 GB
      - Buffer: 58 GB
      
  Network:
    Bandwidth: 500 Mbps minimum
    Inbound: Unlimited
    Outbound: Unlimited (check provider limits)
    
  Cost:
    OVH B2-30: â‚¬0.26/hour (~â‚¬187/month, ~â‚¹17,000/month)
    Hetzner CPX41: â‚¬0.17/hour (~â‚¬122/month, ~â‚¹11,000/month)
    Alternative: AWS t3.2xlarge spot instances (~$80-120/month)
```

### Why This Configuration Works

**CPU-Only Inference Viability:**
- Modern quantized models (Q4_K_M) achieve 15-25 tokens/second on 8-core CPUs
- 30 GB RAM sufficient for 7B parameter models with full context
- No GPU required - reduces cost by 70-80% vs GPU instances
- Proven performance: Llama 2 7B Q4_K_M reaches 15 t/s on AMD EPYC

**Memory Allocation:**
```
Total: 30 GB RAM
â”œâ”€ OS & System: 2 GB
â”œâ”€ Ollama Service: 16 GB (model loading + inference)
â”œâ”€ PostgreSQL: 4 GB
â”œâ”€ Redis: 2 GB
â”œâ”€ API Services: 3 GB
â”œâ”€ Frontend: 1 GB
â””â”€ Buffer: 2 GB
```

---

## Architecture Diagram

### High-Level System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CLIENT LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           React SPA (TypeScript + Vite)                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚   Chat   â”‚  â”‚ Profile  â”‚  â”‚  Resume  â”‚  â”‚   Jobs   â”‚    â”‚  â”‚
â”‚  â”‚  â”‚Interface â”‚  â”‚Dashboard â”‚  â”‚  Builder â”‚  â”‚  Board   â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  State Management: Zustand | API Client: React Query        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ HTTPS/WebSocket
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          API GATEWAY LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Node.js/Express API Gateway (Port 3000)              â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚  â”‚ OAuth Managerâ”‚  â”‚ JWT Handler  â”‚  â”‚Rate Limiter  â”‚      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚         Request Router & Load Balancer              â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚               â”‚               â”‚
                â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MICROSERVICES LAYER                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  AI Service      â”‚  â”‚ Profile Service  â”‚  â”‚  Job Service     â”‚ â”‚
â”‚  â”‚  (Python/FastAPI)â”‚  â”‚ (Node.js/Express)â”‚  â”‚(Node.js/Express) â”‚ â”‚
â”‚  â”‚   Port 8000      â”‚  â”‚   Port 3001      â”‚  â”‚   Port 3002      â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â”‚  LangGraph   â”‚ â”‚  â”‚ â”‚LinkedIn API  â”‚ â”‚  â”‚ â”‚Job Board API â”‚ â”‚
â”‚  â”‚ â”‚ Orchestrator â”‚ â”‚  â”‚ â”‚GitHub API    â”‚ â”‚  â”‚ â”‚Search Engine â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚Resume Parser â”‚ â”‚  â”‚ â”‚Recommender   â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â”‚ Multi-Agent  â”‚ â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â”‚   System     â”‚ â”‚  â”‚ â”‚Profile       â”‚ â”‚  â”‚ â”‚Application   â”‚ â”‚
â”‚  â”‚ â”‚              â”‚ â”‚  â”‚ â”‚Aggregator    â”‚ â”‚  â”‚ â”‚Tracker       â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Supervisor  â”‚ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ â”‚â€¢ Profile     â”‚ â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Skills Gap  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”‚ â”‚â€¢ Job Search  â”‚ â”‚                                             â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Resume      â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚ â”‚
â”‚  â”‚ â”‚â€¢ Learning    â”‚ â”‚  â”‚Learning Service  â”‚                      â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚(Node.js/Express) â”‚                      â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚   Port 3003      â”‚                      â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚                  â”‚                      â”‚ â”‚
â”‚  â”‚ â”‚Ollama Client â”‚ â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                      â”‚ â”‚
â”‚  â”‚ â”‚Integration   â”‚ â”‚  â”‚ â”‚YouTube API   â”‚ â”‚                      â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚GitHub Search â”‚ â”‚                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚Course APIs   â”‚ â”‚                      â”‚ â”‚
â”‚           â”‚            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                      â”‚ â”‚
â”‚           â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚ â”‚
â”‚           â”‚                                                       â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LOCAL LLM INFERENCE LAYER                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Ollama Service (Port 11434)                           â”‚  â”‚
â”‚  â”‚         CPU-Optimized Inference Engine                        â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  Loaded Models (Q4_K_M Quantization)                   â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Qwen2.5:7b-instruct-q4_K_M (~4.5 GB)               â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Supervisor, Resume Builder                        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Benchmark: 18-22 t/s, 74.2 MMLU                 â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Phi3:3.8b-mini-instruct-q4_K_M (~2.5 GB)          â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Profile Analysis, Job Search, Learning           â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Benchmark: 28-32 t/s, fast inference            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Qwen2.5-Coder:7b-instruct-q4_K_M (~4.5 GB)        â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Skills Gap Analysis (technical tasks)            â”‚ â”‚  â”‚
â”‚  â”‚  â”‚    - Benchmark: 20-25 t/s, 57.9 HumanEval            â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  Configuration:                                              â”‚  â”‚
â”‚  â”‚  - OLLAMA_NUM_THREADS=6 (75% of 8 vCores)                  â”‚  â”‚
â”‚  â”‚  - OLLAMA_NUM_GPU=0 (CPU-only mode)                        â”‚  â”‚
â”‚  â”‚  - OLLAMA_MAX_LOADED_MODELS=2 (memory optimization)        â”‚  â”‚
â”‚  â”‚  - Context Length: 2048 tokens (balanced)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA LAYER                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   PostgreSQL     â”‚  â”‚   ChromaDB       â”‚  â”‚     Redis        â”‚ â”‚
â”‚  â”‚  (Primary DB)    â”‚  â”‚ (Vector Store)   â”‚  â”‚   (Cache/Queue)  â”‚ â”‚
â”‚  â”‚   Port 5432      â”‚  â”‚   Port 8001      â”‚  â”‚   Port 6379      â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â€¢ Users          â”‚  â”‚ â€¢ Profile        â”‚  â”‚ â€¢ Sessions       â”‚ â”‚
â”‚  â”‚ â€¢ Profiles       â”‚  â”‚   Embeddings     â”‚  â”‚ â€¢ Chat History   â”‚ â”‚
â”‚  â”‚ â€¢ Conversations  â”‚  â”‚ â€¢ Job            â”‚  â”‚ â€¢ Rate Limits    â”‚ â”‚
â”‚  â”‚ â€¢ Jobs           â”‚  â”‚   Embeddings     â”‚  â”‚ â€¢ Task Queue     â”‚ â”‚
â”‚  â”‚ â€¢ Applications   â”‚  â”‚ â€¢ Skill          â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ â€¢ Resumes        â”‚  â”‚   Embeddings     â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ Storage: 50 GB   â”‚  â”‚ Storage: 10 GB   â”‚  â”‚ Memory: 2 GB     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXTERNAL INTEGRATIONS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ OAuth        â”‚  â”‚  Job Boards  â”‚  â”‚  Learning    â”‚             â”‚
â”‚  â”‚ Providers    â”‚  â”‚              â”‚  â”‚  Platforms   â”‚             â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚             â”‚
â”‚  â”‚ â€¢ Google     â”‚  â”‚ â€¢ LinkedIn   â”‚  â”‚ â€¢ YouTube    â”‚             â”‚
â”‚  â”‚ â€¢ GitHub     â”‚  â”‚ â€¢ Indeed     â”‚  â”‚ â€¢ GitHub     â”‚             â”‚
â”‚  â”‚ â€¢ LinkedIn   â”‚  â”‚ â€¢ Glassdoor  â”‚  â”‚ â€¢ Coursera   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Architecture

### 1. Frontend Architecture

**Technology**: React 18 + TypeScript + Vite

```
Frontend (Port 5173)
â”‚
â”œâ”€â”€ Presentation Layer
â”‚   â”œâ”€â”€ Chat Interface (Real-time streaming)
â”‚   â”œâ”€â”€ Profile Dashboard (Data visualization)
â”‚   â”œâ”€â”€ Resume Builder (Live preview)
â”‚   â”œâ”€â”€ Job Board (Filtering & recommendations)
â”‚   â””â”€â”€ Learning Hub (Resource curation)
â”‚
â”œâ”€â”€ State Management (Zustand)
â”‚   â”œâ”€â”€ Auth Store (JWT, user session)
â”‚   â”œâ”€â”€ Chat Store (Messages, streaming)
â”‚   â”œâ”€â”€ Profile Store (User data, skills)
â”‚   â””â”€â”€ UI Store (Modals, notifications)
â”‚
â””â”€â”€ API Integration (React Query + Axios)
    â”œâ”€â”€ HTTP Client (REST APIs)
    â”œâ”€â”€ WebSocket Client (Chat streaming)
    â””â”€â”€ Caching Layer (React Query)
```

### 2. API Gateway Architecture

**Technology**: Node.js + Express

```
API Gateway (Port 3000)
â”‚
â”œâ”€â”€ Authentication Layer
â”‚   â”œâ”€â”€ OAuth 2.0 Flows (Google, GitHub, LinkedIn)
â”‚   â”œâ”€â”€ JWT Generation & Validation
â”‚   â””â”€â”€ Session Management (Redis)
â”‚
â”œâ”€â”€ Security Layer
â”‚   â”œâ”€â”€ Rate Limiting (Redis-backed)
â”‚   â”œâ”€â”€ Request Validation (Joi)
â”‚   â”œâ”€â”€ CORS Configuration
â”‚   â””â”€â”€ Helmet Security Headers
â”‚
â””â”€â”€ Routing Layer
    â”œâ”€â”€ /api/v1/auth/* â†’ OAuth endpoints
    â”œâ”€â”€ /api/v1/chat/* â†’ AI Service
    â”œâ”€â”€ /api/v1/profile/* â†’ Profile Service
    â”œâ”€â”€ /api/v1/jobs/* â†’ Job Service
    â””â”€â”€ /api/v1/learning/* â†’ Learning Service
```

### 3. AI Service Architecture (CPU-Optimized)

**Technology**: Python + FastAPI + LangGraph + Ollama

```
AI Service (Port 8000)
â”‚
â”œâ”€â”€ Ollama Client Layer
â”‚   â”œâ”€â”€ Model Manager
â”‚   â”‚   â”œâ”€â”€ Load/Unload Models
â”‚   â”‚   â”œâ”€â”€ Memory Optimization
â”‚   â”‚   â””â”€â”€ Thread Configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ Inference Engine
â”‚   â”‚   â”œâ”€â”€ Qwen2.5 7B (Reasoning)
â”‚   â”‚   â”œâ”€â”€ Phi3 Mini (Fast Tasks)
â”‚   â”‚   â””â”€â”€ Qwen2.5-Coder 7B (Technical)
â”‚   â”‚
â”‚   â””â”€â”€ Context Manager
â”‚       â”œâ”€â”€ Token Counting
â”‚       â”œâ”€â”€ Context Window (2048 tokens)
â”‚       â””â”€â”€ Memory Buffer Management
â”‚
â”œâ”€â”€ LangGraph Orchestration Layer
â”‚   â”œâ”€â”€ State Graph Definition
â”‚   â”œâ”€â”€ Node Registration (6 agents)
â”‚   â”œâ”€â”€ Edge Routing Logic
â”‚   â””â”€â”€ Conditional Branching
â”‚
â”œâ”€â”€ Multi-Agent System
â”‚   â”œâ”€â”€ Supervisor Agent
â”‚   â”‚   â””â”€â”€ Uses: Qwen2.5 7B (complex routing)
â”‚   â”‚
â”‚   â”œâ”€â”€ Profile Analysis Agent
â”‚   â”‚   â””â”€â”€ Uses: Phi3 Mini (fast extraction)
â”‚   â”‚
â”‚   â”œâ”€â”€ Skills Gap Agent
â”‚   â”‚   â””â”€â”€ Uses: Qwen2.5-Coder 7B (technical analysis)
â”‚   â”‚
â”‚   â”œâ”€â”€ Job Search Agent
â”‚   â”‚   â””â”€â”€ Uses: Phi3 Mini (fast matching)
â”‚   â”‚
â”‚   â”œâ”€â”€ Resume Builder Agent
â”‚   â”‚   â””â”€â”€ Uses: Qwen2.5 7B (content generation)
â”‚   â”‚
â”‚   â””â”€â”€ Learning Resource Agent
â”‚       â””â”€â”€ Uses: Phi3 Mini (curation)
â”‚
â”œâ”€â”€ Vector Store Integration
â”‚   â”œâ”€â”€ ChromaDB Client
â”‚   â”œâ”€â”€ Embedding Generation (sentence-transformers)
â”‚   â”‚   â””â”€â”€ Model: all-MiniLM-L6-v2 (local, CPU-optimized)
â”‚   â””â”€â”€ Similarity Search
â”‚
â””â”€â”€ API Endpoints
    â”œâ”€â”€ POST /chat (Main conversation endpoint)
    â”œâ”€â”€ POST /resume/generate (JD-matched resume)
    â”œâ”€â”€ POST /skills/analyze (Skills gap analysis)
    â”œâ”€â”€ GET /jobs/recommend (Job recommendations)
    â””â”€â”€ GET /learning/resources (Curated resources)
```

**Critical Ollama Integration Details:**

```python
# integrations/ollama_client.py
from langchain_community.chat_models import ChatOllama
import os

class OllamaClient:
    def __init__(self):
        self.base_url = os.getenv("OLLAMA_BASE_URL", "http://ollama:11434")
        
        # Model for complex reasoning (supervisor, resume)
        self.llm_reasoning = ChatOllama(
            model="qwen2.5:7b-instruct-q4_K_M",
            base_url=self.base_url,
            temperature=0.7,
            num_ctx=2048,      # Context window
            num_thread=6       # 75% of 8 vCores
        )
        
        # Model for fast tasks (profile, jobs, learning)
        self.llm_fast = ChatOllama(
            model="phi3:3.8b-mini-instruct-q4_K_M",
            base_url=self.base_url,
            temperature=0.3,
            num_ctx=2048,
            num_thread=6
        )
        
        # Model for technical/coding tasks (skills gap)
        self.llm_coder = ChatOllama(
            model="qwen2.5-coder:7b-instruct-q4_K_M",
            base_url=self.base_url,
            temperature=0.2,
            num_ctx=2048,
            num_thread=6
        )
    
    def get_model(self, task_type: str):
        """Return appropriate model based on task complexity"""
        if task_type in ["reasoning", "routing", "complex", "resume"]:
            return self.llm_reasoning
        elif task_type in ["coding", "technical", "skills"]:
            return self.llm_coder
        else:
            return self.llm_fast
```

---

## Data Flow

### 1. User Onboarding Flow

```
User Registration
    â†“
OAuth Provider Selection (Google/GitHub/LinkedIn)
    â†“
API Gateway: JWT Token Generation
    â†“
Frontend: Store Token (Zustand + LocalStorage)
    â†“
Profile Service: Create User Record (PostgreSQL)
    â†“
Dashboard: Display Onboarding Steps
```

### 2. Profile Sync Flow

```
User Initiates LinkedIn/GitHub Sync
    â†“
Profile Service: Fetch Data via APIs
    â†“
Profile Service: Send to AI Service for Parsing
    â†“
AI Service: Ollama (Phi3 Mini) Extracts Structured Data
    â†“
Profile Service: Store in PostgreSQL
    â†“
AI Service: Generate Embeddings (sentence-transformers)
    â†“
ChromaDB: Store Profile Vector
    â†“
Frontend: Display Aggregated Profile
```

### 3. Chat Conversation Flow (Multi-Agent)

```
User Sends Message via Chat Interface
    â†“
API Gateway: Validate JWT, Forward to AI Service
    â†“
AI Service: LangGraph Supervisor Agent
    â”‚         (Ollama - Qwen2.5 7B)
    â†“
Supervisor Routes to Appropriate Agent:
    â”‚
    â”œâ”€â†’ Profile Agent (Phi3 Mini) â†’ Fetch User Data
    â”œâ”€â†’ Skills Gap Agent (Qwen2.5-Coder) â†’ Analyze Skills
    â”œâ”€â†’ Job Search Agent (Phi3 Mini) â†’ Match Jobs
    â”œâ”€â†’ Resume Builder Agent (Qwen2.5 7B) â†’ Generate Resume
    â””â”€â†’ Learning Agent (Phi3 Mini) â†’ Curate Resources
    â†“
Agents Execute Tasks (Parallel where possible)
    â†“
Results Aggregated by Supervisor
    â†“
AI Service: Stream Response to Frontend (SSE/WebSocket)
    â†“
Frontend: Display Streaming Message
    â†“
Chat Store: Save to Conversation History
    â†“
PostgreSQL: Persist Conversation
```

---

## Technology Stack

### Frontend Stack

| Category | Technology | Version | Purpose |
|----------|-----------|---------|---------|
| **Framework** | React | 18.3.x | UI library |
| **Language** | TypeScript | 5.x | Type safety |
| **Build Tool** | Vite | 5.x | Fast dev server |
| **State** | Zustand | 4.x | Global state |
| **API Client** | React Query | 5.x | Server state caching |
| **Styling** | TailwindCSS | 3.x | Utility-first CSS |
| **Forms** | React Hook Form | 7.x | Form handling |
| **Validation** | Zod | 3.x | Schema validation |
| **Charts** | Recharts | 2.x | Data visualization |

### Backend Stack

| Category | Technology | Version | Purpose |
|----------|-----------|---------|---------|
| **API Gateway** | Node.js + Express | 20.x / 4.x | HTTP server |
| **AI Service** | Python + FastAPI | 3.11 / 0.104.x | AI endpoints |
| **Microservices** | Node.js + Express | 20.x / 4.x | Business logic |
| **Database** | PostgreSQL | 15.x | Primary data store |
| **Cache** | Redis | 7.x | Session & caching |
| **Vector DB** | ChromaDB | 0.4.x | Embeddings (local, free) |

### AI/ML Stack (CPU-Optimized)

| Component | Technology | Details |
|-----------|-----------|---------|
| **LLM Runtime** | Ollama | 0.1.x (CPU-optimized) |
| **Model 1** | Qwen2.5 7B Instruct | Q4_K_M (~4.5 GB), 74.2 MMLU |
| **Model 2** | Phi3 Mini 3.8B | Q4_K_M (~2.5 GB), 28-32 t/s |
| **Model 3** | Qwen2.5-Coder 7B | Q4_K_M (~4.5 GB), 57.9 HumanEval |
| **Orchestration** | LangGraph | 0.0.x (agent framework) |
| **LangChain** | langchain-community | 0.0.x (Ollama integration) |
| **Embeddings** | sentence-transformers | all-MiniLM-L6-v2 (local) |
| **Vector Store** | ChromaDB Client | Python client |

---

## Deployment Architecture

### Docker Compose Services

```yaml
version: '3.8'

services:
  # Frontend
  frontend:
    image: careergini-frontend:latest
    ports: ["5173:5173"]
    depends_on: [api-gateway]
    
  # API Gateway
  api-gateway:
    image: careergini-api-gateway:latest
    ports: ["3000:3000"]
    depends_on: [postgres, redis, ai-service]
    
  # AI Service (CPU-Optimized)
  ai-service:
    image: careergini-ai-service:latest
    ports: ["8000:8000"]
    depends_on: [ollama, chromadb, postgres]
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    
  # Ollama (Local LLM Engine)
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes: [ollama_models:/root/.ollama]
    environment:
      - OLLAMA_NUM_THREADS=6       # 75% of 8 vCores
      - OLLAMA_NUM_GPU=0            # Force CPU-only
      - OLLAMA_MAX_LOADED_MODELS=2  # Memory optimization
    deploy:
      resources:
        limits:
          cpus: '7'                  # Reserve 7 of 8 cores
          memory: 16G                # 16 GB for Ollama
    
  # PostgreSQL
  postgres:
    image: postgres:15
    ports: ["5432:5432"]
    volumes: [postgres_data:/var/lib/postgresql/data]
    
  # Redis
  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    volumes: [redis_data:/data]
    
  # ChromaDB
  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    ports: ["8001:8000"]
    volumes: [chroma_data:/chroma/chroma]
```

### Resource Allocation

```
Total Server Resources: 8 vCores, 30 GB RAM, 200 GB SSD

CPU Allocation:
â”œâ”€ Ollama:               7 cores (87.5%)
â”œâ”€ AI Service:           0.5 cores (6%)
â”œâ”€ API Gateway:          0.2 cores (2.5%)
â”œâ”€ Microservices:        0.3 cores (4%)
â””â”€ System/Others:        0.0 cores (0%)

RAM Allocation:
â”œâ”€ Ollama:               16 GB (53%)
â”œâ”€ PostgreSQL:           4 GB (13%)
â”œâ”€ AI Service:           3 GB (10%)
â”œâ”€ Redis:                2 GB (7%)
â”œâ”€ ChromaDB:             2 GB (7%)
â”œâ”€ API Gateway:          1 GB (3%)
â”œâ”€ Microservices:        1 GB (3%)
â””â”€ System:               1 GB (3%)
```

---

## Performance Optimization

### CPU-Based LLM Inference Optimization

**1. Model Selection Strategy:**

```
Task-to-Model Mapping:
  Complex Reasoning (Supervisor, Resume):
    Model: Qwen2.5 7B Q4_K_M
    Performance: 18-22 tokens/second
    Memory: ~4.5 GB
    Quality: 74.2 MMLU (SOTA for 7B)
    
  Fast Tasks (Profile, Jobs, Learning):
    Model: Phi3 Mini 3.8B Q4_K_M
    Performance: 28-32 tokens/second
    Memory: ~2.5 GB
    Quality: Good for extraction/classification
    
  Technical Tasks (Skills Gap):
    Model: Qwen2.5-Coder 7B Q4_K_M
    Performance: 20-25 tokens/second
    Memory: ~4.5 GB
    Quality: 57.9 HumanEval (best coding model)
```

**2. Quantization Strategy:**

```
Why Q4_K_M (4-bit K-means Medium):
âœ“ 70% size reduction vs FP16
âœ“ Minimal quality loss (~92% of FP16)
âœ“ 3.5-4x faster inference on CPU
âœ“ Fits multiple models in 30 GB RAM
âœ“ Mixed precision (6-bit attention, 4-bit FFN)
âœ“ Production-proven (Meta, Hugging Face)
```

---

## Cost Analysis

### Monthly Operating Costs (Estimated)

```
Infrastructure:
â”œâ”€ OVH B2-30 Server:           â‚¬187/month (~â‚¹17,000)
â”œâ”€ Domain + SSL:               â‚¬5/month (~â‚¹450)
â”œâ”€ Backups (100 GB):           â‚¬10/month (~â‚¹900)
â””â”€ TOTAL INFRASTRUCTURE:       â‚¬202/month (~â‚¹18,350)

LLM Costs:
â”œâ”€ Ollama (Local):             â‚¬0/month âœ“
â”œâ”€ OpenAI (Alternative):       â‚¬200-500/month âŒ
â””â”€ TOTAL LLM COSTS:            â‚¬0/month

GRAND TOTAL:                   â‚¬202/month (~â‚¹18,350)

Cost Comparison (100K requests/month):
CareerGini (Local LLMs):       â‚¬202/month
Alternative (OpenAI GPT-4):    â‚¬1,200/month
Alternative (Anthropic Claude): â‚¬900/month

Savings: 83% cheaper than paid LLM APIs
```

---

**Document Version**: 2.0 (CPU-Optimized for B2-30)  
**Last Updated**: February 17, 2026  
**Maintained By**: CareerGini Development Team